# DQL_cartpole
- 기간: 2022.07

# 왜 시작했을까?
- 추천시스템에서 현재 각광받고 있는 알고리즘은 강화학습(Reinforcement Learning; RL)과 그래프 네트워크(Graph Neural Network;GNN)이다. 개인적으로 모두 처음 들어보는 알고리즘이어서, 관련 페이퍼를 읽어보았다. 강화학습의 경우, 어느정도 통계학 기반으로 진행되어서 쉽게 읽을 수 있었고, GNN의 경우에는 수리적 기반 지식이 부족해서 완전하게 이해하지 못했다. 또, 강화학습은 first paper가 짧기도 하고, 검색을 통해서 논문을 완전히 이해하였지만, GNN은 first paper가 30페이지였나? 그정도여서 읽을 엄두가 안났다. 그래서 기본적으로 알고 있던 CNN과 결합된 GCN논문부터 읽는게 수월할 거라고 생각했는데, 이러한 생각이 아주 큰 오산이었다. GCN에서 나오는 GNN 알고리즘 설명 부분이 매우 많이 생략되어 있었고, 오히려 한번도 배워본 적이 없었던 XAI, 설명가능한 인공지능과 결합된 GNNeXplainer 페이퍼에서 기본적인 GNN 구조를 이해할 수 있었다. 어쨌든 그렇게 강화학습 첫번째 페이퍼를 읽으면서 강화학습에 대한 이론적인 기반을 배웠고, 딥러닝과 결합되면서 아주 우수한 성능을 갖는 DQN 알고리즘을 알게 되었다. 아마 추천시스템에서 각광받고 있는 것도 이 때문인것 같다. 딥러닝과 강화학습이 결합되면서, 추천시스템이라는 새로운 분야에서 각광받는 것 같다. 따라서, 강화학습을 이론적인 배경에서만 이해하는 것이 아니라, 실제 코드로 구현해보면서 이해도를 높이고 싶었다. 

- 하지만, 문제가 생겼다. 추천 시스템과 관련한 데이터가 나에게는 없었고, 크롤링까지도 생각해보았지만, 다른 일도 갑자기 많이 생겨서 그렇게까지 하지는 못했다. 그래서 우선은 강화학습이 많이 사용되는 게임 분야(Cartpole)의 코드들만 구현해보았고, baseline과 DQN 모두 구현해보았다!
 - 생각해보니까 머신러닝에서 IRIS데이터 처럼, 추천시스템에서도 기초 데이터인 movielens 데이터가 있었다! 추천시스템을 페이퍼 속에서만 보고, 직접 구현한 지는 굉장히 오래되어서 다 까먹고 있었던 거다... 조만간 무비렌즈 데이터셋을 기반으로 여러 추천시스템 알고리즘을 구현해봐야지!!
 
## What is Cartpole?
![image](https://jonghyunho.github.io/assets/img/posts/20200505/cartpole_episode_100.gif)
1. 마찰이 없는 track에 cart가 하나 있습니다. 
2. Cart는 pole 하나가 세워져 있고, cart에 힘을 가해 pole이 넘어지지 않도록 조작합니다. 
3. cart에 +1 혹은 -1의 힘을 가할 수 있습니다. 
4. Pole이 제대로 세워져 있으면 매 time-step 마다 +1의 보상을 받습니다. 
 
 # Baseline PseudoCode
 ![image](https://user-images.githubusercontent.com/77769026/177915827-798e8b1b-8724-4b8c-b471-be66f2339d39.png)


# DQN PseudoCode
![image](https://user-images.githubusercontent.com/77769026/177915870-8ccbd1ec-b151-480c-9a04-aa5485def995.png)

# 아쉬운 점
- 다음에는 추천시스템 데이터를 크롤링 해서 추천시스템에 직접 적용되게 구현하봐야지
- 이번에는 사실 다른 사람들이 구현해 놓은 코드를 가져와서 실행해보고 이해하는 것에서 그쳤지만, 다음번에는 새로운 데이터에서 내가 로우레벨부터 구현해보고 싶다.
- + 궁금한점, 강화학습에서 보상이라는 개념은 매우매우 중요한 것으로, 알고리즘을 업데이터 하는데 쓰이는 데, 추천시스템에서 "보상"은 어떻게 정의되며, 계산은 어떻게 할 수 있을 까?
- 다음에는 강화학습말고, GNN까지도 구현하고 이해해봐야지!

끝.
